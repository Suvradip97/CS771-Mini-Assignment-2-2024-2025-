{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi-wsebcrs5C"
      },
      "outputs": [],
      "source": [
        "# Filename: cs771_mp2_lwp_pipeline.py\n",
        "# Put this in a notebook (.ipynb) or run as a script after adjusting paths.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, datasets, models\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "# -------------------------\n",
        "# Config / Hyperparameters\n",
        "# -------------------------\n",
        "DATA_ROOT = \"data\"   # where you put downloaded D1..D20 and heldout folders\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 128\n",
        "FEAT_BACKBONE = \"resnet18\"   # options: resnet18, resnet50\n",
        "USE_PCA = True               # reduce feature dim (stabilizes prototype distances)\n",
        "PCA_DIM = 256\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Task1 (same distribution) hyperparams (more aggressive pseudo-labeling)\n",
        "TASK1_CONF_THRESH = 0.7\n",
        "TASK1_ALPHA = 0.25   # EMA update weight for prototypes\n",
        "# Task2 (different distributions) hyperparams (conservative)\n",
        "TASK2_CONF_THRESH = 0.9\n",
        "TASK2_ALPHA = 0.08\n",
        "TASK2_DISTANCE_FACTOR = 1.5  # allows update only if mean distance <= factor * class_std\n",
        "\n",
        "# -------------------------\n",
        "# Utilities: image loader\n",
        "# -------------------------\n",
        "imagenet_mean = [0.485, 0.456, 0.406]\n",
        "imagenet_std  = [0.229, 0.224, 0.225]\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),            # backbone expects larger images than CIFAR\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
        "])\n",
        "\n",
        "def list_image_files(folder):\n",
        "    \"\"\"Return full-path list of image files (non-recursive).\"\"\"\n",
        "    exts = {\".png\", \".jpg\", \".jpeg\", \".bmp\"}\n",
        "    files = [os.path.join(folder, f) for f in os.listdir(folder)\n",
        "             if os.path.splitext(f.lower())[1] in exts]\n",
        "    files.sort()\n",
        "    return files\n",
        "\n",
        "class GenericImageDataset(Dataset):\n",
        "    \"\"\"Handles both labeled (class-subfolders) and unlabeled (all images) cases.\"\"\"\n",
        "    def __init__(self, root_folder, transform=None, labeled=True):\n",
        "        self.root = root_folder\n",
        "        self.transform = transform\n",
        "        self.labeled = labeled\n",
        "        # detect labeled structure (class subfolders) automatically\n",
        "        if labeled:\n",
        "            # Use torchvision.ImageFolder semantics if there are subfolders\n",
        "            # We'll detect whether the folder contains subfolders with images\n",
        "            subfolders = [os.path.join(root_folder,d) for d in os.listdir(root_folder)\n",
        "                          if os.path.isdir(os.path.join(root_folder,d))]\n",
        "            has_classes = False\n",
        "            for sf in subfolders:\n",
        "                if list_image_files(sf):\n",
        "                    has_classes = True\n",
        "                    break\n",
        "            if has_classes:\n",
        "                # Build a flat list with class indices from folder names\n",
        "                classes = sorted([d for d in os.listdir(root_folder) if os.path.isdir(os.path.join(root_folder,d))])\n",
        "                self.class_to_idx = {c:i for i,c in enumerate(classes)}\n",
        "                self.samples = []\n",
        "                for c in classes:\n",
        "                    for f in list_image_files(os.path.join(root_folder,c)):\n",
        "                        self.samples.append((f, self.class_to_idx[c]))\n",
        "            else:\n",
        "                # no labeled subfolders -> treat as unlabeled\n",
        "                self.labeled = False\n",
        "                self.samples = [(f, -1) for f in list_image_files(root_folder)]\n",
        "        else:\n",
        "            self.samples = [(f, -1) for f in list_image_files(root_folder)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fpath, lbl = self.samples[idx]\n",
        "        img = Image.open(fpath).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, lbl, fpath\n",
        "\n",
        "# -------------------------\n",
        "# Feature extractor\n",
        "# -------------------------\n",
        "def build_backbone(name=\"resnet18\", device=\"cpu\"):\n",
        "    if name == \"resnet18\":\n",
        "        model = models.resnet18(pretrained=True)\n",
        "        feat_dim = model.fc.in_features\n",
        "    elif name == \"resnet50\":\n",
        "        model = models.resnet50(pretrained=True)\n",
        "        feat_dim = model.fc.in_features\n",
        "    else:\n",
        "        raise ValueError(\"Unknown backbone\")\n",
        "    # remove final fc\n",
        "    modules = list(model.children())[:-1]\n",
        "    feat_net = nn.Sequential(*modules)\n",
        "    feat_net.to(device).eval()\n",
        "    return feat_net, feat_dim\n",
        "\n",
        "def extract_features_for_folder(folder, backbone, batch_size=128, transform=transform, labeled=True, device=\"cpu\"):\n",
        "    ds = GenericImageDataset(folder, transform=transform, labeled=labeled)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    feats = []\n",
        "    labels = []\n",
        "    paths = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls, fpaths in tqdm(dl, desc=f\"Feat extract {os.path.basename(folder)}\"):\n",
        "            imgs = imgs.to(device)\n",
        "            out = backbone(imgs)\n",
        "            out = out.view(out.size(0), -1).cpu().numpy()\n",
        "            feats.append(out)\n",
        "            labels.extend([int(x) for x in lbls])\n",
        "            paths.extend(fpaths)\n",
        "    feats = np.vstack(feats)\n",
        "    labels = np.array(labels)   # -1 for unlabeled\n",
        "    return feats, labels, paths\n",
        "\n",
        "# -------------------------\n",
        "# LwP classifier\n",
        "# -------------------------\n",
        "def softmax_rows(x):\n",
        "    x = x - x.max(axis=1, keepdims=True)\n",
        "    e = np.exp(x)\n",
        "    return e / e.sum(axis=1, keepdims=True)\n",
        "\n",
        "class LwPClassifier:\n",
        "    \"\"\"\n",
        "    Prototype-based classifier.\n",
        "    prototypes: numpy array (C, D)\n",
        "    counts: pseudo-counts used for Bayesian mean update (keeps model param fixed size)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, feat_dim, use_prob=False):\n",
        "        self.C = num_classes\n",
        "        self.D = feat_dim\n",
        "        self.use_prob = use_prob\n",
        "        self.prototypes = np.zeros((self.C, self.D), dtype=np.float32)\n",
        "        self.counts = np.zeros((self.C,), dtype=np.float32) + 1e-6\n",
        "        # For probabilistic variant keep diagonal variances\n",
        "        if self.use_prob:\n",
        "            self.var = np.ones((self.C, self.D), dtype=np.float32)\n",
        "\n",
        "    def fit(self, feats, labels):\n",
        "        \"\"\"Fit prototypes from labeled data (feats: N x D, labels: N).\"\"\"\n",
        "        for c in range(self.C):\n",
        "            idx = np.where(labels == c)[0]\n",
        "            if len(idx) > 0:\n",
        "                self.prototypes[c] = feats[idx].mean(axis=0)\n",
        "                self.counts[c] = len(idx)\n",
        "\n",
        "    def predict_proba(self, feats, temperature=1.0):\n",
        "        \"\"\"Return class probabilities via negative Euclidean distances + softmax.\"\"\"\n",
        "        # feats: N x D\n",
        "        # compute squared euclid dist: (a-b)^2 = a^2 + b^2 - 2ab\n",
        "        a2 = np.sum(feats * feats, axis=1, keepdims=True)   # N x 1\n",
        "        b2 = np.sum(self.prototypes * self.prototypes, axis=1)   # C\n",
        "        ab = feats.dot(self.prototypes.T)   # N x C\n",
        "        d2 = a2 + b2 - 2*ab   # N x C\n",
        "        scores = -np.sqrt(np.maximum(d2, 0.0)) / max(1e-8, temperature)  # negative distance\n",
        "        probs = softmax_rows(scores)\n",
        "        return probs, d2  # returns distances also for optional gating\n",
        "\n",
        "    def predict(self, feats):\n",
        "        probs, d2 = self.predict_proba(feats)\n",
        "        preds = probs.argmax(axis=1)\n",
        "        confs = probs.max(axis=1)\n",
        "        return preds, confs, d2\n",
        "\n",
        "    def update_with_pseudo(self, feats, preds, confs, distances=None,\n",
        "                           conf_thresh=0.7, alpha=0.2, conservative_mask=None,\n",
        "                           distance_factor=1.5):\n",
        "        \"\"\"\n",
        "        Updates prototypes using pseudo-labeled samples.\n",
        "        - Only samples with conf >= conf_thresh are used.\n",
        "        - alpha controls EMA (prototype <- (1-alpha)*proto + alpha*mean_new )\n",
        "        - conservative_mask (optional): boolean per-class specifying whether to be extra conservative\n",
        "        - distance_factor: for conservative mode, only update if mean distance <= distance_factor * class_std\n",
        "        \"\"\"\n",
        "        # compute per-class updates\n",
        "        for c in range(self.C):\n",
        "            idx = np.where(preds == c)[0]\n",
        "            if len(idx) == 0:\n",
        "                continue\n",
        "            # select confident subset\n",
        "            idx_conf = idx[confs[idx] >= conf_thresh]\n",
        "            if len(idx_conf) == 0:\n",
        "                continue\n",
        "            # compute mean of confident features\n",
        "            new_mean = feats[idx_conf].mean(axis=0)\n",
        "            # optional conservative gating by distance: require new_mean close to prototype\n",
        "            if distances is not None:\n",
        "                # distances provided as d2 (N x C)\n",
        "                mean_dist = np.sqrt(np.mean(distances[idx_conf, c]))\n",
        "                # compute per-class std from current prototypes via distances to proto - approximate\n",
        "                # approximate class std: sqrt(mean of squared dist for samples assigned to class using current proto)\n",
        "                # we use counts to estimate stability; fallback to small value if counts low\n",
        "                class_std = 0.0\n",
        "                if self.counts[c] > 1:\n",
        "                    # approximate by computing prot-dist for prior pseudo-samples - we don't have them; use a heuristic\n",
        "                    class_std = max(1.0, np.sqrt(np.mean(distances[:, c])))  # fallback heuristic\n",
        "                else:\n",
        "                    class_std = max(1.0, mean_dist)\n",
        "                if mean_dist > distance_factor * class_std:\n",
        "                    # skip update as cluster is too far -> likely wrong pseudo-labels\n",
        "                    continue\n",
        "            # update via EMA while preserving proto shape (keeps parameter count same)\n",
        "            self.prototypes[c] = (1.0 - alpha) * self.prototypes[c] + alpha * new_mean\n",
        "            # bump counts (we do not access original labeled data; counts are part of model params)\n",
        "            self.counts[c] += len(idx_conf)\n",
        "\n",
        "    def save(self, path):\n",
        "        with open(path, \"wb\") as f:\n",
        "            pickle.dump({\"prototypes\": self.prototypes, \"counts\": self.counts}, f)\n",
        "\n",
        "    def load(self, path):\n",
        "        with open(path, \"rb\") as f:\n",
        "            d = pickle.load(f)\n",
        "        self.prototypes = d[\"prototypes\"]\n",
        "        self.counts = d[\"counts\"]\n",
        "\n",
        "# -------------------------\n",
        "# High-level training loops for Task1 and Task2\n",
        "# -------------------------\n",
        "def evaluate_on_heldouts(model, heldout_feats_dict, heldout_labels_dict, upto_index):\n",
        "    \"\"\"\n",
        "    Evaluate model on D_hat1..D_hat_{upto_index} (1-based indexing).\n",
        "    heldout_feats_dict: {\"D_hat1\": feats, ...}\n",
        "    returns list of accuracies for datasets 1..upto_index\n",
        "    \"\"\"\n",
        "    accs = []\n",
        "    for i in range(1, upto_index+1):\n",
        "        key = f\"D_hat{i}\"\n",
        "        feats = heldout_feats_dict[key]\n",
        "        labels = heldout_labels_dict[key]\n",
        "        preds, confs, _ = model.predict(feats)\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "def run_task1(backbone, feat_dim, data_root=DATA_ROOT,\n",
        "              use_pca=USE_PCA, pca_dim=PCA_DIM):\n",
        "    \"\"\"\n",
        "    Implements Task 1 (D1..D10), returns accuracy matrix (10x10)\n",
        "    \"\"\"\n",
        "    # 1) Precompute features for D1..D10 and heldouts D_hat1..D_hat10\n",
        "    train_feats = {}\n",
        "    train_labels = {}\n",
        "    for i in range(1, 11):\n",
        "        folder = os.path.join(data_root, f\"D{i}\")\n",
        "        feats, labels, paths = extract_features_for_folder(folder, backbone, batch_size=BATCH_SIZE, transform=transform, labeled=True, device=DEVICE)\n",
        "        train_feats[f\"D{i}\"] = feats\n",
        "        train_labels[f\"D{i}\"] = labels\n",
        "\n",
        "    held_feats = {}\n",
        "    held_labels = {}\n",
        "    for i in range(1, 11):\n",
        "        folder = os.path.join(data_root, \"heldout\", f\"D_hat{i}\")\n",
        "        feats, labels, paths = extract_features_for_folder(folder, backbone, batch_size=BATCH_SIZE, transform=transform, labeled=True, device=DEVICE)\n",
        "        held_feats[f\"D_hat{i}\"] = feats\n",
        "        held_labels[f\"D_hat{i}\"] = labels\n",
        "\n",
        "    # Optional PCA fit on D1 features (or all D1..D10 labeled if available)\n",
        "    if use_pca:\n",
        "        pca = TruncatedSVD(n_components=pca_dim, random_state=0)\n",
        "        # fit PCA on D1 labeled features only (per assignment D1 is the only labeled among first 10)\n",
        "        pca.fit(train_feats[\"D1\"])\n",
        "        for k in list(train_feats.keys()):\n",
        "            train_feats[k] = pca.transform(train_feats[k])\n",
        "        for k in list(held_feats.keys()):\n",
        "            held_feats[k] = pca.transform(held_feats[k])\n",
        "        feat_dim = pca_dim\n",
        "    else:\n",
        "        pca = None\n",
        "\n",
        "    # Initialize model f1 from D1\n",
        "    model = LwPClassifier(NUM_CLASSES, feat_dim)\n",
        "    model.fit(train_feats[\"D1\"], train_labels[\"D1\"])\n",
        "\n",
        "    # We'll store prototype snapshot after each step\n",
        "    models_snapshots = {}\n",
        "    accuracy_matrix = np.zeros((10,10), dtype=float)\n",
        "\n",
        "    # sequential update for D1..D10\n",
        "    for i in range(1, 11):\n",
        "        # model is f_i\n",
        "        # Evaluate on heldouts 1..i\n",
        "        accs = evaluate_on_heldouts(model, held_feats, held_labels, upto_index=i)\n",
        "        accuracy_matrix[i-1, :i] = accs\n",
        "        print(f\"f{i} accuracies on D_hat1..D_hat{i} = {np.round(accs,4)}\")\n",
        "        models_snapshots[f\"f{i}\"] = {\"prototypes\": model.prototypes.copy(), \"counts\": model.counts.copy()}\n",
        "        # if last, break (no D_{i+1} to update from)\n",
        "        if i == 10:\n",
        "            break\n",
        "        # predict on next dataset D_{i+1} (unlabeled) and update\n",
        "        feats_next = train_feats[f\"D{i+1}\"]\n",
        "        preds, confs, d2 = model.predict(feats_next)\n",
        "        # update with pseudo labels using TASK1 aggressive hyperparams\n",
        "        model.update_with_pseudo(feats_next, preds, confs, distances=d2,\n",
        "                                 conf_thresh=TASK1_CONF_THRESH, alpha=TASK1_ALPHA)\n",
        "        print(f\"Updated f{i} -> f{i+1} using D{i+1} pseudo-labels.\")\n",
        "    return model, models_snapshots, accuracy_matrix, pca\n",
        "\n",
        "def run_task2(backbone, feat_dim, starting_model_snapshot, data_root=DATA_ROOT,\n",
        "              use_pca=USE_PCA):\n",
        "    \"\"\"\n",
        "    Implements Task 2 (starting from f10) over D11..D20.\n",
        "    starting_model_snapshot: a dict with 'prototypes' and 'counts' from f10\n",
        "    returns accuracy matrix (10 x 20) for f11..f20 on D_hat1..D_hat20\n",
        "    \"\"\"\n",
        "    # Precompute D11..D20 features and heldouts D_hat1..D_hat20 (we need heldouts 1..20)\n",
        "    train_feats = {}\n",
        "    for i in range(11, 21):\n",
        "        folder = os.path.join(data_root, f\"D{i}\")\n",
        "        feats, labels, paths = extract_features_for_folder(folder, backbone, batch_size=BATCH_SIZE, transform=transform, labeled=False, device=DEVICE)\n",
        "        train_feats[f\"D{i}\"] = feats\n",
        "\n",
        "    held_feats = {}\n",
        "    held_labels = {}\n",
        "    for i in range(1, 21):\n",
        "        folder = os.path.join(data_root, \"heldout\", f\"D_hat{i}\")\n",
        "        feats, labels, paths = extract_features_for_folder(folder, backbone, batch_size=BATCH_SIZE, transform=transform, labeled=True, device=DEVICE)\n",
        "        held_feats[f\"D_hat{i}\"] = feats\n",
        "        held_labels[f\"D_hat{i}\"] = labels\n",
        "\n",
        "    # If PCA was used in Task1, you must transform the new features using same PCA (passed as param).\n",
        "    # (Assume caller will apply PCA transform to these feats if needed)\n",
        "\n",
        "    # Initialize model f10 from snapshot\n",
        "    model = LwPClassifier(NUM_CLASSES, feat_dim)\n",
        "    model.prototypes = starting_model_snapshot[\"prototypes\"].copy()\n",
        "    model.counts = starting_model_snapshot[\"counts\"].copy()\n",
        "\n",
        "    accuracy_matrix = np.zeros((10, 20), dtype=float)\n",
        "    models_snapshots = {}\n",
        "\n",
        "    # sequential updates D11..D20 producing f11..f20\n",
        "    for idx, i in enumerate(range(11, 21), start=1):\n",
        "        # update model using current dataset D_i to produce f_i\n",
        "        feats_next = train_feats[f\"D{i}\"]\n",
        "        preds, confs, d2 = model.predict(feats_next)\n",
        "\n",
        "        # Adaptive conservative scheme:\n",
        "        # - compute class-wise mean confidence; only update classes with high mean confidence\n",
        "        # - compute mean distance to prototype and require it not to be too large (gating)\n",
        "        mean_confidence_per_class = []\n",
        "        for c in range(NUM_CLASSES):\n",
        "            idx_c = np.where(preds == c)[0]\n",
        "            if len(idx_c) == 0:\n",
        "                mean_confidence_per_class.append(0.0)\n",
        "            else:\n",
        "                mean_confidence_per_class.append(float(confs[idx_c].mean()))\n",
        "        mean_confidence_per_class = np.array(mean_confidence_per_class)\n",
        "\n",
        "        # Build conservative mask: only classes with mean_conf >= TASK2_CONF_THRESH\n",
        "        conservative_mask = mean_confidence_per_class >= TASK2_CONF_THRESH\n",
        "\n",
        "        # Use distance gating as well: compute mean sqrt distance for confident predicted samples per class\n",
        "        # We'll pass distances into update method to implement gating\n",
        "        model.update_with_pseudo(feats_next, preds, confs, distances=d2,\n",
        "                                 conf_thresh=TASK2_CONF_THRESH, alpha=TASK2_ALPHA,\n",
        "                                 conservative_mask=conservative_mask,\n",
        "                                 distance_factor=TASK2_DISTANCE_FACTOR)\n",
        "        # After update we have f_i (i between 11..20)\n",
        "        model_idx = idx  # 1..10\n",
        "        # Evaluate f_i on D_hat1..D_hat{i} (all previous heldouts)\n",
        "        upto = i  # D_hat1..D_hati (but accuracy matrix expects columns as D_hat1..D_hat20)\n",
        "        # compute accuracies on D_hat1..D_hat{i}\n",
        "        accs = []\n",
        "        for j in range(1, i+1):\n",
        "            feats = held_feats[f\"D_hat{j}\"]\n",
        "            labels = held_labels[f\"D_hat{j}\"]\n",
        "            preds_h, confs_h, _ = model.predict(feats)\n",
        "            accs.append(accuracy_score(labels, preds_h))\n",
        "        # Fill into accuracy matrix: row model_idx-1, columns 0..i-1\n",
        "        accuracy_matrix[model_idx-1, :i] = accs\n",
        "        models_snapshots[f\"f{i}\"] = {\"prototypes\": model.prototypes.copy(), \"counts\": model.counts.copy()}\n",
        "        print(f\"f{i} updated and evaluated on heldouts 1..{i}: {np.round(accs,4)}\")\n",
        "\n",
        "    return model, models_snapshots, accuracy_matrix\n",
        "\n",
        "# -------------------------\n",
        "# Example main driver\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Build backbone\n",
        "    backbone, feat_dim = build_backbone(FEAT_BACKBONE, device=DEVICE)\n",
        "    # Run Task1\n",
        "    final_model_f10, snaps_task1, acc_mat_task1, pca = run_task1(backbone, feat_dim, data_root=DATA_ROOT)\n",
        "    print(\"Task1 accuracy matrix (10x10):\")\n",
        "    print(np.round(acc_mat_task1, 4))\n",
        "    # Take f10 snapshot for Task2:\n",
        "    f10_snapshot = snaps_task1[\"f10\"]\n",
        "    # If PCA was used in Task1, apply same PCA to D11..D20 and heldouts in Task2\n",
        "    if USE_PCA and pca is not None:\n",
        "        # transform D11..D20 and heldout features using pca inside run_task2 or pre-transform files\n",
        "        # For simplicity this script currently assumes run_task2 will read raw feats and you should apply the same `pca.transform`\n",
        "        pass\n",
        "    # Run Task2 (user must ensure D11..D20/heldout features are PCA-transformed if PCA used)\n",
        "    final_model_f20, snaps_task2, acc_mat_task2 = run_task2(backbone, feat_dim, f10_snapshot, data_root=DATA_ROOT, use_pca=USE_PCA)\n",
        "    print(\"Task2 accuracy matrix (10x20) for f11..f20 rows and D_hat1..D_hat20 columns\")\n",
        "    print(np.round(acc_mat_task2, 4))\n",
        "    # Save matrices\n",
        "    np.save(\"acc_mat_task1.npy\", acc_mat_task1)\n",
        "    np.save(\"acc_mat_task2.npy\", acc_mat_task2)\n",
        "    # Save prototype snapshots\n",
        "    joblib.dump(snaps_task1, \"snaps_task1.pkl\")\n",
        "    joblib.dump(snaps_task2, \"snaps_task2.pkl\")\n",
        "    print(\"Saved results: acc matrices and snapshots.\")\n"
      ]
    }
  ]
}